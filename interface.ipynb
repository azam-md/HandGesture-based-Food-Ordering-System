{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45da9f99-ad96-4377-b4b7-ec6abad05eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4942df7e-cad0-4cd1-8d8a-6fda2e69c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13bdef-83f9-42f0-8fca-2c9a4067f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_hand_landmarks(image):\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            lmx = [landmark.x for landmark in landmarks.landmark]\n",
    "            lmy = [landmark.y for landmark in landmarks.landmark]\n",
    "            min_x, max_x, min_y, max_y = int(min(lmx)*image.shape[1]), int(max(lmx)*image.shape[1]), int(min(lmy)*image.shape[0]), int(max(lmy)*image.shape[0])\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            \n",
    "            # Indicate left or right hand\n",
    "            hand_label = results.multi_handedness[index].classification[0].label\n",
    "            hand_label = \"Right\" if hand_label == \"Left\" else \"Left\"\n",
    "            cv2.putText(image, hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    return None\n",
    "\n",
    "def landmarks_to_heatmap(landmarks, img_size=(128, 128), blob_size=5):\n",
    "    heatmap = np.zeros(img_size)\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * img_size[0]), int(landmark[1] * img_size[1])\n",
    "        cv2.circle(heatmap, (x, y), blob_size, (255, 255, 255), -1)\n",
    "    return heatmap\n",
    "\n",
    "def preprocess_image(img, img_size=(128, 128)):\n",
    "    landmarks = extract_hand_landmarks(img)\n",
    "    if landmarks:\n",
    "        heatmap = landmarks_to_heatmap(landmarks, img_size)\n",
    "        return heatmap / 255.0\n",
    "    return None\n",
    "feature_extractor = load_model(\"cnn_model_3_mediapipe_90.h5\")\n",
    "dt_classifier = joblib.load('dt_classifier_3_mediapipe_90.pkl')\n",
    "unique_labels = joblib.load('unique_labels_3_mediapipe_90.pkl')\n",
    "img_size=(128,128)\n",
    "#def real_time_prediction(n):\n",
    "def real_time_prediction(model, feature_extractor):\n",
    "       # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "    # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        scale_factor = 1.5  # Adjust this value as needed\n",
    "        new_width = int(frame.shape[1] * scale_factor)\n",
    "        new_height = int(frame.shape[0] * scale_factor)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    # Check if frame captured successfully\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Preprocess the frame\n",
    "        processed_img = preprocess_image(resized_frame, img_size)\n",
    "    \n",
    "        if processed_img is not None:\n",
    "            processed_img_reshaped = processed_img.reshape((-1, img_size[0], img_size[1], 1))\n",
    "        \n",
    "        # Extract features from CNN\n",
    "            features = feature_extractor.predict(processed_img_reshaped)\n",
    "        \n",
    "        # Predict using the Decision Tree classifier\n",
    "            prediction = dt_classifier.predict(features)\n",
    "            \n",
    "            # Convert integer label back to string label\n",
    "            predicted_gesture = unique_labels[prediction[0]]\n",
    "        \n",
    "        # Display the result on the frame\n",
    "        # Instead of a fixed position, compute the position dynamically\n",
    "            text_position = (int(resized_frame.shape[1] * 0.05), int(resized_frame.shape[0] * 0.05))\n",
    "\n",
    "            cv2.putText(resized_frame, predicted_gesture, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "    # Display the frame\n",
    "        cv2.imshow('Gesture Recognition', resized_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "#real_time_prediction(5)\n",
    "real_time_prediction(dt_classifier, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a452dc-f6ce-4b5d-b66a-c2690877800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878d518c-7eea-4371-ae58-665e3b9d0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d40e1-80c8-4f10-bcc8-4a88e0af5baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b6d5c6-ae05-4928-91fd-e70c5fbbf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a7cc25-1b66-400c-b1ac-409568641dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, Response, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfaf6d4-c45a-415a-8033-ea17fde7edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "'''\n",
    "def extract_hand_landmarks(image):\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    return None'''\n",
    "last_landmarks = None\n",
    "last_bounding_box = None\n",
    "last_hand_label = None\n",
    "def extract_hand_landmarks(image):\n",
    "    global last_landmarks, last_bounding_box, last_hand_label\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Update bounding box\n",
    "            lmx = [landmark.x for landmark in landmarks.landmark]\n",
    "            lmy = [landmark.y for landmark in landmarks.landmark]\n",
    "            min_x, max_x, min_y, max_y = int(min(lmx)*image.shape[1]), int(max(lmx)*image.shape[1]), int(min(lmy)*image.shape[0]), int(max(lmy)*image.shape[0])\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            \n",
    "            # Update hand label\n",
    "            hand_label = results.multi_handedness[index].classification[0].label\n",
    "            hand_label = \"Right\" if hand_label == \"Left\" else \"Left\"\n",
    "            cv2.putText(image, hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            # Store current landmarks, bounding box and label for future frames\n",
    "            last_landmarks = landmarks\n",
    "            last_bounding_box = (min_x, min_y, max_x, max_y)\n",
    "            last_hand_label = hand_label\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    else:\n",
    "        # If no hand is detected, use the last detected landmarks, bounding box, and label\n",
    "        if last_landmarks and last_bounding_box:\n",
    "            mp_drawing.draw_landmarks(image, last_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            min_x, min_y, max_x, max_y = last_bounding_box\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            if last_hand_label:\n",
    "                cv2.putText(image, last_hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return None\n",
    "\n",
    "def landmarks_to_heatmap(landmarks, img_size=(128, 128), blob_size=5):\n",
    "    heatmap = np.zeros(img_size)\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * img_size[0]), int(landmark[1] * img_size[1])\n",
    "        cv2.circle(heatmap, (x, y), blob_size, (255, 255, 255), -1)\n",
    "    return heatmap\n",
    "\n",
    "def preprocess_image(img, img_size=(128, 128)):\n",
    "    landmarks = extract_hand_landmarks(img)\n",
    "    if landmarks:\n",
    "        heatmap = landmarks_to_heatmap(landmarks, img_size)\n",
    "        return heatmap / 255.0\n",
    "    return None\n",
    "\n",
    "feature_extractor = load_model(\"cnn_model_3_mediapipe_90.h5\")\n",
    "dt_classifier = joblib.load('dt_classifier_3_mediapipe_90.pkl')\n",
    "unique_labels = joblib.load('unique_labels_3_mediapipe_90.pkl')\n",
    "img_size = (128, 128)\n",
    "path = \"D:/B Tech/IV Year/project/project new/dataset_mediapipe\"\n",
    "def generate():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "       # resized_frame = cv2.resize(frame, (int(frame.shape[1]*1.5), int(frame.shape[0]*1.5)))\n",
    "        processed_img = preprocess_image(frame, img_size)\n",
    "    \n",
    "        if processed_img is not None:\n",
    "            processed_img_reshaped = processed_img.reshape((-1, img_size[0], img_size[1], 1))\n",
    "            features = feature_extractor.predict(processed_img_reshaped)\n",
    "            prediction = dt_classifier.predict(features)\n",
    "            predicted_gesture = unique_labels[prediction[0]]\n",
    "            final_output = os.path.basename(predicted_gesture)\n",
    "            text_position = (int(frame.shape[1] * 0.05), int(frame.shape[0] * 0.05))\n",
    "            cv2.putText(frame, final_output, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "        _, jpeg = cv2.imencode('.jpg', frame)\n",
    "        yield (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + jpeg.tobytes() + b'\\r\\n\\r\\n')\n",
    "@app.route('/get_gesture')\n",
    "def get_gesture():\n",
    "    # Your gesture recognition logic here\n",
    "    recognized_gesture = \"gesture_name\"  # Replace with actual recognized gesture\n",
    "    return jsonify(gesture=recognized_gesture)\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(generate(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index2.html')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7578d9-cdb2-45a6-a5c9-7ecf25967065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
