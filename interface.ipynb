{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ed270b6-b044-4f98-bc34-d170a79ffe5d",
   "metadata": {},
   "source": [
    "#import joblib\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_hand_landmarks(image):\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            lmx = [landmark.x for landmark in landmarks.landmark]\n",
    "            lmy = [landmark.y for landmark in landmarks.landmark]\n",
    "            min_x, max_x, min_y, max_y = int(min(lmx)*image.shape[1]), int(max(lmx)*image.shape[1]), int(min(lmy)*image.shape[0]), int(max(lmy)*image.shape[0])\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            \n",
    "            # Indicate left or right hand\n",
    "            hand_label = results.multi_handedness[index].classification[0].label\n",
    "            hand_label = \"Right\" if hand_label == \"Left\" else \"Left\"\n",
    "            cv2.putText(image, hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    return None\n",
    "\n",
    "def landmarks_to_heatmap(landmarks, img_size=(128, 128), blob_size=5):\n",
    "    heatmap = np.zeros(img_size)\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * img_size[0]), int(landmark[1] * img_size[1])\n",
    "        cv2.circle(heatmap, (x, y), blob_size, (255, 255, 255), -1)\n",
    "    return heatmap\n",
    "\n",
    "def preprocess_image(img, img_size=(128, 128)):\n",
    "    landmarks = extract_hand_landmarks(img)\n",
    "    if landmarks:\n",
    "        heatmap = landmarks_to_heatmap(landmarks, img_size)\n",
    "        return heatmap / 255.0\n",
    "    return None\n",
    "feature_extractor = load_model(\"cnn_model_3_mediapipe_90.h5\")\n",
    "dt_classifier = joblib.load('dt_classifier_3_mediapipe_90.pkl')\n",
    "unique_labels = joblib.load('unique_labels_3_mediapipe_90.pkl')\n",
    "img_size=(128,128)\n",
    "#def real_time_prediction(n):\n",
    "def real_time_prediction(model, feature_extractor):\n",
    "       # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "    # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        scale_factor = 1.5  # Adjust this value as needed\n",
    "        new_width = int(frame.shape[1] * scale_factor)\n",
    "        new_height = int(frame.shape[0] * scale_factor)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    # Check if frame captured successfully\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Preprocess the frame\n",
    "        processed_img = preprocess_image(resized_frame, img_size)\n",
    "    \n",
    "        if processed_img is not None:\n",
    "            processed_img_reshaped = processed_img.reshape((-1, img_size[0], img_size[1], 1))\n",
    "        \n",
    "        # Extract features from CNN\n",
    "            features = feature_extractor.predict(processed_img_reshaped)\n",
    "        \n",
    "        # Predict using the Decision Tree classifier\n",
    "            prediction = dt_classifier.predict(features)\n",
    "            \n",
    "            # Convert integer label back to string label\n",
    "            predicted_gesture = unique_labels[prediction[0]]\n",
    "        \n",
    "        # Display the result on the frame\n",
    "        # Instead of a fixed position, compute the position dynamically\n",
    "            text_position = (int(resized_frame.shape[1] * 0.05), int(resized_frame.shape[0] * 0.05))\n",
    "\n",
    "            cv2.putText(resized_frame, predicted_gesture, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "    # Display the frame\n",
    "        cv2.imshow('Gesture Recognition', resized_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "#real_time_prediction(5)\n",
    "real_time_prediction(dt_classifier, feature_extractor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b5e7393-f6e0-43e2-b03a-3af199f1b084",
   "metadata": {},
   "source": [
    "app = Flask(_name_)\n",
    "\n",
    "# Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "'''\n",
    "def extract_hand_landmarks(image):\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    return None'''\n",
    "last_landmarks = None\n",
    "last_bounding_box = None\n",
    "last_hand_label = None\n",
    "def extract_hand_landmarks(image):\n",
    "    global last_landmarks, last_bounding_box, last_hand_label\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Update bounding box\n",
    "            lmx = [landmark.x for landmark in landmarks.landmark]\n",
    "            lmy = [landmark.y for landmark in landmarks.landmark]\n",
    "            min_x, max_x, min_y, max_y = int(min(lmx)*image.shape[1]), int(max(lmx)*image.shape[1]), int(min(lmy)*image.shape[0]), int(max(lmy)*image.shape[0])\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            \n",
    "            # Update hand label\n",
    "            hand_label = results.multi_handedness[index].classification[0].label\n",
    "            hand_label = \"Right\" if hand_label == \"Left\" else \"Left\"\n",
    "            cv2.putText(image, hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            # Store current landmarks, bounding box and label for future frames\n",
    "            last_landmarks = landmarks\n",
    "            last_bounding_box = (min_x, min_y, max_x, max_y)\n",
    "            last_hand_label = hand_label\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    else:\n",
    "        # If no hand is detected, use the last detected landmarks, bounding box, and label\n",
    "        if last_landmarks and last_bounding_box:\n",
    "            mp_drawing.draw_landmarks(image, last_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            min_x, min_y, max_x, max_y = last_bounding_box\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            if last_hand_label:\n",
    "                cv2.putText(image, last_hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return None\n",
    "\n",
    "def landmarks_to_heatmap(landmarks, img_size=(128, 128), blob_size=5):\n",
    "    heatmap = np.zeros(img_size)\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * img_size[0]), int(landmark[1] * img_size[1])\n",
    "        cv2.circle(heatmap, (x, y), blob_size, (255, 255, 255), -1)\n",
    "    return heatmap\n",
    "\n",
    "def preprocess_image(img, img_size=(128, 128)):\n",
    "    landmarks = extract_hand_landmarks(img)\n",
    "    if landmarks:\n",
    "        heatmap = landmarks_to_heatmap(landmarks, img_size)\n",
    "        return heatmap / 255.0\n",
    "    return None\n",
    "\n",
    "feature_extractor = load_model(\"cnn_model_3_mediapipe_90.h5\")\n",
    "dt_classifier = joblib.load('dt_classifier_3_mediapipe_90.pkl')\n",
    "unique_labels = joblib.load('unique_labels_3_mediapipe_90.pkl')\n",
    "img_size = (128, 128)\n",
    "path = \"D:/B Tech/IV Year/project/project new/dataset_mediapipe\"\n",
    "def generate():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "       # resized_frame = cv2.resize(frame, (int(frame.shape[1]*1.5), int(frame.shape[0]*1.5)))\n",
    "        processed_img = preprocess_image(frame, img_size)\n",
    "    \n",
    "        if processed_img is not None:\n",
    "            processed_img_reshaped = processed_img.reshape((-1, img_size[0], img_size[1], 1))\n",
    "            features = feature_extractor.predict(processed_img_reshaped)\n",
    "            prediction = dt_classifier.predict(features)\n",
    "            predicted_gesture = unique_labels[prediction[0]]\n",
    "            final_output = os.path.basename(predicted_gesture)\n",
    "            text_position = (int(frame.shape[1] * 0.05), int(frame.shape[0] * 0.05))\n",
    "            cv2.putText(frame, final_output, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "        _, jpeg = cv2.imencode('.jpg', frame)\n",
    "        yield (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + jpeg.tobytes() + b'\\r\\n\\r\\n')\n",
    "@app.route('/get_gesture')\n",
    "def get_gesture():\n",
    "    # Your gesture recognition logic here\n",
    "    recognized_gesture = final_output  # Replace with actual recognized gesture\n",
    "    return jsonify(gesture=recognized_gesture)\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(generate(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index2.html')\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4942df7e-cad0-4cd1-8d8a-6fda2e69c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a452dc-f6ce-4b5d-b66a-c2690877800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832d40e1-80c8-4f10-bcc8-4a88e0af5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_socketio import SocketIO, emit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b6d5c6-ae05-4928-91fd-e70c5fbbf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a7cc25-1b66-400c-b1ac-409568641dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, Response, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627b6be-dadb-47b6-98c3-c07a8e7c210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47539e96-2305-4417-a02d-eb92dd5bc676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6607b4b2-4baa-4ffd-a99d-ed4d85062bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Werkzeug appears to be used in a production deployment. Consider switching to a production web server instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:51] \"GET /static/icon0.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/js/your_script.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/icon1.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/icon2.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/icon3.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/menu2.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/background.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /static/icon4.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"GET /socket.io/?EIO=4&transport=polling&t=Oj6fhL3 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:52] \"POST /socket.io/?EIO=4&transport=polling&t=Oj6fhNS&sid=R8THpsnRh064_WAdAAAA HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:53] \"GET /static/icon5.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:53] \"GET /static/icon6.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:53] \"GET /socket.io/?EIO=4&transport=polling&t=Oj6fhNW&sid=R8THpsnRh064_WAdAAAA HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:06:53] \"GET /socket.io/?EIO=4&transport=polling&t=Oj6fhX6&sid=R8THpsnRh064_WAdAAAA HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:07:01] \"GET /video_feed HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Oct/2023 12:08:00] \"GET /static/quantity1.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:00] \"GET /static/quantity2.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:00] \"GET /static/quantity3.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:00] \"GET /static/quantity5.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:00] \"GET /static/quantity4.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:13] \"GET /socket.io/?EIO=4&transport=websocket&sid=R8THpsnRh064_WAdAAAA HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:39] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:41] \"GET /static/icon0.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/icon1.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/icon2.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/js/your_script.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/icon3.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/background.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/menu2.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /static/icon4.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /socket.io/?EIO=4&transport=polling&t=Oj6g69X HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"POST /socket.io/?EIO=4&transport=polling&t=Oj6g6D7&sid=-6jbqsoUWR5QRqFZAAAC HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:42] \"GET /socket.io/?EIO=4&transport=polling&t=Oj6g6DH&sid=-6jbqsoUWR5QRqFZAAAC HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:44] \"GET /static/icon5.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:44] \"GET /static/icon6.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:08:47] \"GET /video_feed HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2023 12:09:19] \"GET /socket.io/?EIO=4&transport=websocket&sid=-6jbqsoUWR5QRqFZAAAC HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app)\n",
    "# Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "'''\n",
    "def extract_hand_landmarks(image):\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    return None'''\n",
    "last_landmarks = None\n",
    "last_bounding_box = None\n",
    "last_hand_label = None\n",
    "def extract_hand_landmarks(image):\n",
    "    global last_landmarks, last_bounding_box, last_hand_label\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Update bounding box\n",
    "            lmx = [landmark.x for landmark in landmarks.landmark]\n",
    "            lmy = [landmark.y for landmark in landmarks.landmark]\n",
    "            min_x, max_x, min_y, max_y = int(min(lmx)*image.shape[1]), int(max(lmx)*image.shape[1]), int(min(lmy)*image.shape[0]), int(max(lmy)*image.shape[0])\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            \n",
    "            # Update hand label\n",
    "            hand_label = results.multi_handedness[index].classification[0].label\n",
    "            hand_label = \"Right\" if hand_label == \"Left\" else \"Left\"\n",
    "            cv2.putText(image, hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            # Store current landmarks, bounding box and label for future frames\n",
    "            last_landmarks = landmarks\n",
    "            last_bounding_box = (min_x, min_y, max_x, max_y)\n",
    "            last_hand_label = hand_label\n",
    "        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "        landmarks_array = [[landmark.x, landmark.y, landmark.z] for landmark in landmarks]\n",
    "        return landmarks_array\n",
    "    else:\n",
    "        # If no hand is detected, use the last detected landmarks, bounding box, and label\n",
    "        if last_landmarks and last_bounding_box:\n",
    "            mp_drawing.draw_landmarks(image, last_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            min_x, min_y, max_x, max_y = last_bounding_box\n",
    "            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "            if last_hand_label:\n",
    "                cv2.putText(image, last_hand_label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return None\n",
    "\n",
    "def landmarks_to_heatmap(landmarks, img_size=(128, 128), blob_size=5):\n",
    "    heatmap = np.zeros(img_size)\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * img_size[0]), int(landmark[1] * img_size[1])\n",
    "        cv2.circle(heatmap, (x, y), blob_size, (255, 255, 255), -1)\n",
    "    return heatmap\n",
    "\n",
    "def preprocess_image(img, img_size=(128, 128)):\n",
    "    landmarks = extract_hand_landmarks(img)\n",
    "    if landmarks:\n",
    "        heatmap = landmarks_to_heatmap(landmarks, img_size)\n",
    "        return heatmap / 255.0\n",
    "    return None\n",
    "\n",
    "feature_extractor = load_model(\"cnn_model_3_mediapipe_90.h5\")\n",
    "dt_classifier = joblib.load('dt_classifier_3_mediapipe_90.pkl')\n",
    "unique_labels = joblib.load('unique_labels_3_mediapipe_90.pkl')\n",
    "img_size = (128, 128)\n",
    "path = \"D:/B Tech/IV Year/project/project new/dataset_mediapipe\"\n",
    "#def generate():\n",
    "\n",
    "@socketio.on('start_gesture_recognition')\n",
    "def get_gesture():\n",
    "    # Your gesture recognition logic here\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "       # resized_frame = cv2.resize(frame, (int(frame.shape[1]*1.5), int(frame.shape[0]*1.5)))\n",
    "        processed_img = preprocess_image(frame, img_size)\n",
    "    \n",
    "        if processed_img is not None:\n",
    "            processed_img_reshaped = processed_img.reshape((-1, img_size[0], img_size[1], 1))\n",
    "            features = feature_extractor.predict(processed_img_reshaped)\n",
    "            prediction = dt_classifier.predict(features)\n",
    "            predicted_gesture = unique_labels[prediction[0]]\n",
    "            final_output = os.path.basename(predicted_gesture)\n",
    "            text_position = (int(frame.shape[1] * 0.05), int(frame.shape[0] * 0.05))\n",
    "            cv2.putText(frame, final_output, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "            socketio.emit('gesture_update', final_output)\n",
    "        _, jpeg = cv2.imencode('.jpg', frame)\n",
    "        yield (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + jpeg.tobytes() + b'\\r\\n\\r\\n')\n",
    "\n",
    "        #emit('gesture_update', (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + jpeg.tobytes() + b'\\r\\n\\r\\n').decode('utf-8'))\n",
    "       # socketio.emit('frame_update', jpeg.tobytes().decode('base64'))\n",
    "        \n",
    "\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(get_gesture(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index2.html')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, allow_unsafe_werkzeug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb334f-9db8-4c62-941f-5ce3066008c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
